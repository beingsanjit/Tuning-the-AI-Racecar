# ğŸï¸ Tuning the AI Racecar

A Generative AI experiment inspired by Formula 1 engineering â€” testing how small prompt tweaks and design decisions affect LLM performance, accuracy, and latency.

## ğŸ“Œ Overview

In Formula 1, a 2mm change in front wing angle can mean the difference between pole position and crashing out. This project explores how similar â€œmicro-decisionsâ€ in GenAI â€” like rewording a prompt or changing context window size â€” produce outsized effects on a systemâ€™s behavior.

## ğŸ¯ Goal

Use **Vertex AI**, **LangChain**, and structured prompt experimentation to:

- Identify how subtle prompt tweaks affect hallucination rate
- Measure latency and cost trade-offs under different settings
- Visualize prompt performance like lap times on an F1 circuit
- Encourage systems-level thinking in GenAI application design

## ğŸ› ï¸ Tech Stack

- ğŸ§  Vertex AI / Gemini
- ğŸ”— LangChain
- ğŸ“Š BigQuery (optional for logging)
- ğŸ“ˆ Python + Matplotlib for visualizations
- ğŸ““ Jupyter / Google Colab

## ğŸ“ Repo Structure

- `notebooks/` â†’ all experiments and evaluation runs
- `data/` â†’ prompts, context documents, configs
- `results/` â†’ output generations, latency logs, plots
- `paper/` â†’ academic-style writeup (for arXiv or PDF)

## ğŸš€ Roadmap

- [ ] First experiment: prompt tone vs hallucination rate
- [ ] Benchmarking latency + token usage
- [ ] Radar charts comparing prompt types
- [ ] Medium article + GitHub showcase
- [ ] Optional arXiv submission (structured paper format)

## ğŸ“– Inspired By

- *How to Build a Car* by Adrian Newey
- OpenAI, Anthropic, and Google AI system papers

## ğŸ‘‹ Author

Sanjit Sharma â€” technology consultant, systems thinker, and machine learning engineer in the making.

https://www.linkedin.com/in/sanjitsharma1/
